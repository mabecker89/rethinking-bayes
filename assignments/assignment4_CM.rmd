---
title: "Assignment4_CM"
author: "Christopher Mallon"
date: "13/02/2022"
output: html_document
---


```{r Setup}
rm(list = ls())
gc()
library(rethinking)
```

# Question 1
1. Consider three fictional Polynesian islands. On each there is a Royal Or-
nithologist charged by the king with surveying the birb population. They
have each found the following proportions of 5 important birb species:

Island | Birb A | Birb B | Birb C | Birb D | Birb E
-------|-------|-------|-------|-------|-------|
Island 1 | 0.2  | 0.2  | 0.2   | 0.2   | 0.2
Island 2 | 0.8  | 0.1  | 0.05  | 0.025 | 0.025
Island 3 | 0.05 | 0.15 | 0.7   | 0.05  | 0.05

Notice that each row sums to 1, all the birbs. This problem has two parts. It
is not computationally complicated. But it is conceptually tricky.
First, compute the entropy of each island’s birb distribution. Interpret
these entropy values.
Second, use each island’s birb distribution to predict the other two. This
means to compute the K-L Divergence of each island from the others, treat-
ing each island as if it were a statistical model of the other islands. You should
end up with 6 different K-L Divergence values. Which island predicts the
others best? Why?

```{r Question 1 - entropy}

get_entropy <- function(probs){
  log_probs = -sum(probs * log(probs))
  return(log_probs)
}

i1 <- c(0.2, 0.2, 0.2, 0.2, 0.2)
i2 <- c(0.8, 0.1, 0.05, 0.025, 0.025)
i3 <- c(0.05, 0.15, 0.7, 0.05, 0.05)

entropies <- data.frame("island" = c(1,2,3), 'entropy' = c(get_entropy(i1), get_entropy(i2), get_entropy(i3)))
knitr::kable(entropies)

```
The entropy values are highest when the distribution is widest and least peaked. As such, 
island 1 has the largest entropy value. This is a bit counter-intuitive!

```{r Question 1- KL Divergences}
get_kl_divergence <- function(p,q){
  H.p <- get_entropy(p)
  H.p.q <- -sum(p*log(q))
  return(H.p.q - H.p)
}

d <- list('i1' = c(0.2, 0.2, 0.2, 0.2, 0.2),
          'i2' = c(0.8, 0.1, 0.05, 0.025, 0.025),
          'i3' = c(0.05, 0.15, 0.7, 0.05, 0.05))
out <- data.frame('p' = numeric(), 'q' = numeric(), 'kld' = numeric())
for (i in c('i1', 'i2', 'i3')){
  for (j in c('i1', 'i2', 'i3')){
    kld <- get_kl_divergence(d[[i]], d[[j]])
    out <- rbind(out, data.frame('p' = i, 'q' = j, 'kld' = kld))
  }
}

knitr::kable(out)

```
The first island is the winner, because it has the highest entropy it is able
to predict the others the most successfully. In other words, someone who is used
to island 1 would be least surprised by the other islands as compared to someone
from island 2 or 3. Neat!


# Question 2
2. Recall the marriage, age, and happiness collider bias example from Chap-
ter 6. Run models m6.9 and m6.10 again. Compare these two models using
WAIC (or LOO, they will produce identical results). Which model is ex-
pected to make better predictions? Which model provides the correct causal
inference about the influence of age on happiness? Can you explain why the
answers to these two questions disagree?

```{r Question 2 - WAIC values of models}
d <- sim_happiness( seed=1977 , N_years=1000 )

## R code 6.22
d2 <- d[ d$age>17 , ] # only adults
d2$A <- ( d2$age - 18 ) / ( 65 - 18 )

## R code 6.23
## This model includes marriage status and age, and concludes age ~ 1/happiness
d2$mid <- d2$married + 1
m6.9 <- quap(
    alist(
        happiness ~ dnorm( mu , sigma ),
        mu <- a[mid] + bA*A,
        a[mid] ~ dnorm( 0 , 1 ),
        bA ~ dnorm( 0 , 2 ),
        sigma ~ dexp(1)
    ) , data=d2 )
#precis(m6.9,depth=2)

## R code 6.24
## Excludes marriage status, no association between age and happiness
m6.10 <- quap(
    alist(
        happiness ~ dnorm( mu , sigma ),
        mu <- a + bA*A,
        a ~ dnorm( 0 , 1 ),
        bA ~ dnorm( 0 , 2 ),
        sigma ~ dexp(1)
    ) , data=d2 )
#precis(m6.10)

## Marriage is a collider. When we condition on it, we introduce a fictional
# link between the causes, when really it is a result of age and happiness.

compare(m6.9, m6.10)

```
So strangely, model 6.9, which conditions on a collider, is expected to make
more accurate predictions. However, 6.10 simulates the correct causal chain.
I'm only guessing here, but the main difference is that by using a[min] the author
creates a categorical variable to calculate the y-intercept for twice. As a result,
there is a smaller body of observations for each cateogry, and maybe therefore
an artificially higher WAIC?

# Question 3
3. Reconsider the urban fox analysis from last week’s homework. Use WAIC
or LOO based model comparison on five different models, each using weight
as the outcome, and containing these sets of predictor variables:

```
(1) avgfood + groupsize + area
(2) avgfood + groupsize
(3) groupsize + area
(4) avgfood
(5) area
```

Can you explain the relative differences in WAIC scores, using the fox DAG
from last week’s homework? Be sure to pay attention to the standard error
of the score differences ( dSE ).

```{r Question 3 - Regularization}
data(foxes)
fx <- foxes
fx$W <- (fx$weight - mean(fx$weight)) / sd(fx$weight)
fx$AF <- (fx$avgfood - mean(fx$avgfood)) / sd(fx$avgfood)
fx$GS <- (fx$groupsize - mean(fx$groupsize)) / sd(fx$groupsize)
fx$A <- (fx$area - mean(fx$area)) / sd(fx$area)

```

```{r Question 3 - Models}
f.1 <- quap(
  alist(
    W ~ dnorm(mu, sigma),
    mu <- b1*AF + b2*GS + b3*A + alpha,
    alpha ~ dnorm(0,1),
    b1 ~ dnorm(0,0.5),
    b2 ~ dnorm(0,0.5),
    b3 ~ dnorm(0,0.5),
    sigma ~ dexp(1)
  ), data = fx
)

f.2 <- quap(
  alist(
    W ~ dnorm(mu, sigma),
    mu <- b1*AF + b2*GS + alpha,
    alpha ~ dnorm(0,1),
    b1 ~ dnorm(0,0.5),
    b2 ~ dnorm(0,0.5),
    sigma ~ dexp(1)
  ), data = fx
)

f.3 <- quap(
  alist(
    W ~ dnorm(mu, sigma),
    mu <- b1*GS + b2*A + alpha,
    alpha ~ dnorm(0,1),
    b1 ~ dnorm(0,0.5),
    b2 ~ dnorm(0,0.5),
    sigma ~ dexp(1)
  ), data = fx
)

f.4 <- quap(
  alist(
    W ~ dnorm(mu, sigma),
    mu <- b1*AF + alpha,
    alpha ~ dnorm(0,1),
    b1 ~ dnorm(0,0.5),
    sigma ~ dexp(1)
  ), data = fx
)

f.5 <- quap(
  alist(
    W ~ dnorm(mu, sigma),
    mu <- b1*A + alpha,
    alpha ~ dnorm(0,1),
    b1 ~ dnorm(0,0.5),
    sigma ~ dexp(1)
  ), data = fx
)

f.6 <- quap(
  alist(
    W ~ dnorm(mu, sigma),
    mu <- b1*GS + alpha,
    alpha ~ dnorm(0,1),
    b1 ~ dnorm(0,0.5),
    sigma ~ dexp(1)
  ), data = fx
)

compare(f.1, f.2, f.3, f.4, f.5, f.6)

```

So from what I can tell, none of these models have substantially more predictive
power than the others. That's because dSE*pWAIC > dWAIC for each model. We know
that weight is a collider, but we haven't conditioned on it. We also know that groupsize is a
pipe, but again, haven't conditioned on it in every model. The models without 
are area and average food, and they come out slightly worse despite having
fewer parameters. Nope, no idea what's happening here. Yikes!
